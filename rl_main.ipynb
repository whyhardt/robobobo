{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!apt-get update && apt-get install swig cmake\n",
    "!pip install box2d-py\n",
    "!pip install \"stable-baselines3[extra]>=2.0.0a4\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import gym\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "\n",
    "from stable_baselines3 import PPO, SAC, DDPG, TD3\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "from training import simple_test\n",
    "from environment import Environment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"main file for training and testing the SAC-agent on the environment\"\"\"\n",
    "\n",
    "# warnings off\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "cfg = {\n",
    "    # general parameters\n",
    "    'load_checkpoint': False,\n",
    "    'file_checkpoint': os.path.join('trained_rl', 'checkpoint.pt'),\n",
    "    'file_data': os.path.join('stock_data', 'stocks_sp20_2010_2020.csv'),\n",
    "    'file_predictor': [None, None],  # ['trained_gan/real_gan_1k.pt', 'trained_gan/mvgavg_gan_10k.pt',],\n",
    "    'checkpoint_interval': 10,\n",
    "\n",
    "    # training parameters\n",
    "    'train': True,\n",
    "    'agent': 'ppo',\n",
    "    'env_id': \"Custom\",  # Custom, Pendulum-v1, MountainCarContinuous-v0, LunarLander-v2\n",
    "    'num_epochs': 1e1,\n",
    "    'num_actions_per_epoch': 1e2,\n",
    "    'num_random_actions': 5e2,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 3e-4,\n",
    "    'temperature': 0.0001,\n",
    "    'train_test_split': 0.8,\n",
    "    'replay_buffer_size': 1e6,\n",
    "    'parameter_update_interval': 50,\n",
    "    'polyak': 0.995,\n",
    "    'gamma': 0.99,\n",
    "\n",
    "    # network parameters\n",
    "    'hidden_dim': 256,\n",
    "    'num_layers': 3,\n",
    "    'num_layers_sub': 4,\n",
    "    'init_w': None,\n",
    "    'dropout': 0.0,\n",
    "\n",
    "    # environment\n",
    "    'time_limit': 365,\n",
    "    'cash_init': 10000,\n",
    "    'commission': .001,\n",
    "    'observation_length': 16,\n",
    "    'reward_scaling': 1e-4,\n",
    "}\n",
    "\n",
    "list_valid_agents = ['sac', 'ddpg', 'td3', 'ppo']\n",
    "assert cfg['agent'] in list_valid_agents, f\"Agent must be one of: {list_valid_agents}\"\n",
    "agent_dict = {'sac': (lambda policy, env: SAC(policy, env, verbose=1),\n",
    "                      lambda path: SAC.load(path)),\n",
    "              'ddpg': (lambda policy, env: DDPG(policy, env, verbose=1),\n",
    "                       lambda path: DDPG.load(path)),\n",
    "              'td3': (lambda policy, env: TD3(policy, env, verbose=1),\n",
    "                      lambda path: TD3.load(path)),\n",
    "              'ppo': (lambda policy, env: PPO(policy, env, verbose=1),\n",
    "                      lambda path: PPO.load(path)),\n",
    "                }\n",
    "\n",
    "\n",
    "print('Initializing framework...')\n",
    "\n",
    "# load data\n",
    "training_data = pd.read_csv(cfg['file_data'], index_col=0, header=0).to_numpy(dtype=np.float32)\n",
    "test_data = training_data[int(cfg['train_test_split']*len(training_data)):]\n",
    "training_data = training_data[:int(cfg['train_test_split']*len(training_data))]\n",
    "\n",
    "# load environment\n",
    "if cfg['env_id'] == 'Custom':\n",
    "    env = Environment(training_data, cfg['cash_init'], cfg['observation_length'], time_limit=cfg['time_limit'])\n",
    "    # env = TimeLimit(env, max_episode_steps=cfg['time_limit'])\n",
    "else:\n",
    "    env = gym.make(cfg['env_id'], render_mode=\"human\")\n",
    "\n",
    "# load agent\n",
    "if not cfg['load_checkpoint']:\n",
    "    agent = agent_dict[cfg['agent']][0]('MlpPolicy', env)\n",
    "    print(f\"Agent {cfg['agent']} initialized!\")\n",
    "else:\n",
    "    agent = agent_dict[cfg['agent']][1](cfg['file_checkpoint'])\n",
    "    print(f\"Agent {cfg['agent']} from path {cfg['file_checkpoint']} loaded!\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# train RL framework\n",
    "# --------------------------------------------\n",
    "\n",
    "if cfg['train']:\n",
    "    avg_rewards, avg_stds = [], []\n",
    "    for i in range(int(cfg['num_epochs'])):\n",
    "        agent.learn(total_timesteps=cfg['num_actions_per_epoch'], log_interval=10)\n",
    "        avg_reward, avg_std = evaluate_policy(agent, env, n_eval_episodes=5)\n",
    "        avg_rewards.append(avg_reward)\n",
    "        avg_stds.append(avg_std)\n",
    "        print(f\"Epoch {i}/{int(cfg['num_epochs'])}: avg_reward={avg_reward:.2f} +/- {avg_std:.2f}\")\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # plot results\n",
    "    # --------------------------------------------\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(avg_rewards)\n",
    "    plt.fill_between(range(len(avg_rewards)), np.array(avg_rewards)-np.array(avg_stds), np.array(avg_rewards)+np.array(avg_stds), alpha=0.2)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Average reward')\n",
    "    plt.show()\n",
    "\n",
    "# --------------------------------------------\n",
    "# save agent\n",
    "# --------------------------------------------\n",
    "\n",
    "# current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# path = os.path.join('trained_rl', f'{cfg[\"agent\"]}_{cfg[\"env_id\"]}_{current_time}')\n",
    "# agent.save(path)\n",
    "# print(f\"Agent saved to {path}\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# test RL framework\n",
    "# --------------------------------------------\n",
    "env = Environment(test_data, cfg['cash_init'], cfg['observation_length'], time_limit=-1,)\n",
    "simple_test(env, agent, test=True, plot_reference=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
